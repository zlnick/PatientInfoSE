from dotenv import load_dotenv
import os
import uuid
from openai import AsyncOpenAI
from mcp import ClientSession
import chainlit as cl
import json
from utils import parse_mcp_result,get_result_value,get_practitioner,get_official_name
from context_manager import IRISContextManager
from planner_agent import generate_plan
from context_aware_agent import can_answer_from_context, generate_context_answer
from data_visualization_agent import generate_interactive_plotly_chart
import audioop
import numpy as np
import io
import wave
import dashscope
from dashscope import MultiModalConversation
import base64

load_dotenv()

prac_id = os.getenv("Practioner_ID")
practioner = get_practitioner(prac_id)
prac_name = get_official_name(practioner)
assistant_name = os.getenv("Assistant_NAME")
llm_model = os.getenv("LLM_MODEL")

#ä¸´åºŠåŠ©æ‰‹Prompt
assistant_prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸´åºŠåŒ»ç”Ÿçš„é—¨è¯ŠåŠ©æ‰‹ã€‚ä½ å°†ç”¨åŒ»ç”Ÿå®¹æ˜“é˜…è¯»çš„è‡ªç„¶è¯­è¨€å’ŒåŒ»ç”Ÿç”¨ä¸­æ–‡äº¤æµã€‚ä¸è¦å‘åŒ»ç”Ÿè¿”å›å¯¹FHIRã€SQLè¡¨ç­‰æ•°æ®çš„æŠ€æœ¯ä¿¡æ¯ï¼Œä½ å°†ä¼šå°†è¿™äº›ä¿¡æ¯ç”¨è‡ªç„¶è¯­è¨€æè¿°åå†å‘åŒ»ç”Ÿåé¦ˆã€‚
ä½ éå¸¸äº†è§£HL7 FHIRåè®®ï¼ŒçŸ¥é“idæˆ–èµ„æºidæŒ‡çš„æ˜¯idå‚æ•°ã€‚
ä½ è¿˜çŸ¥é“å¦‚ä¸‹ä¸´åºŠçŸ¥è¯†ï¼š
è¡€å‹çš„å­—å…¸ç æ˜¯85354-9ã€‚
å½“æ¥æ”¶åˆ°ä¸€æ‰¹åŒä¸€æ‚£è€…çš„æ•°æ®æ—¶ï¼Œé™¤äº†å‘åŒ»ç”Ÿæè¿°æ‚£è€…æ•°æ®ï¼Œè¿˜åº”ä»ä¸´åºŠè§’åº¦è¿›è¡Œæ€»ç»“ã€‚
å½“åŒ»ç”Ÿè¯•å›¾ä¸ºæ‚£è€…å¼€è¯æˆ–è¯¢é—®è¯ç‰©æ˜¯å¦é€‚ç”¨æ—¶ï¼Œä½ åº”å½“å…ˆæ£€æŸ¥ä¸Šä¸‹æ–‡ï¼ŒæŸ¥çœ‹æ˜¯å¦å·²é€šè¿‡FHIR Patientèµ„æºçš„$everythingæ“ä½œè·å¾—äº†æ‚£è€…çš„æ‰€æœ‰ä¿¡æ¯ã€‚å¦‚æœæœ‰ï¼Œåˆ™ç»“åˆæ‚£è€…ä¿¡æ¯å’Œè¯ç‰©ä¿¡æ¯å›ç­”é—®é¢˜ï¼›å¦‚æœæ²¡æœ‰ï¼Œåˆ™å…ˆé€šè¿‡$everythingæ“ä½œè·å–æ‚£è€…çš„å®Œæ•´æ¡£æ¡ˆï¼Œå†ç»“åˆè¯å“ä¿¡æ¯å›ç­”é—®é¢˜ã€‚
"""

#è¯ç‰©åŒ»ä¿æŠ¥é”€é£é™©æ£€æµ‹åŠ©æ‰‹ Prompt
insurance_expert_prompt = f"""
åŸºäºä¸Šä¸‹æ–‡ä¿¡æ¯ä¸­çš„ä»¥ä¸‹ä¿¡æ¯å›ç­”åŒ»ä¿æ‹’ä»˜é£é™©ç›¸å…³çš„é—®é¢˜ã€‚ä¸Šä¸‹æ–‡ä¸­åŒ…å«ï¼š
1. ç¬¬ä¸€ä¸ªå…ƒç´ çš„ä¿¡æ¯é€šå¸¸ä¸ºæ‚£è€…ä¿¡æ¯ï¼Œåº”è¯¥ä¸ºé€šè¿‡FHIRçš„$everythingæ“ä½œè·å¾—çš„å®Œæ•´æ¡£æ¡ˆï¼Œä½†ä½ åœ¨æè¿°ä¿¡æ¯æ—¶ä¸è¦æåˆ°FHIRï¼Œåªè¯´æ˜¯æ‚£è€…æ¡£æ¡ˆä¸­çš„ä¿¡æ¯å³å¯ã€‚
2. ä¹‹åçš„ä¿¡æ¯ä¸ºé€šè¿‡çŸ¥è¯†åº“æŸ¥è¯¢è·å¾—çš„ï¼Œä¸é—®é¢˜ä¸­æåˆ°çš„è¯ç‰©æœ€ç›¸å…³çš„åŒ»ä¿è§„åˆ™ä¿¡æ¯

éµå¾ªåŸåˆ™:
1. è¦ä¾æ®è·å–åˆ°çš„åŒ»ä¿è§„åˆ™ä»”ç»†åˆ†æï¼Œç¡®è®¤æŠ¥é”€çº¦æŸæ˜¯ä¸æ˜¯éƒ½å¾—åˆ°äº†æ»¡è¶³ã€‚
    ä¾‹å¦‚ç›é…¸å³ç¾æ‰˜å’ªå®šçš„æŠ¥é”€æ¡ä»¶ä¸ºï¼šæˆäººæœ¯å‰é•‡é™/æŠ—ç„¦è™‘ï¼Œåˆ™åªæœ‰æ‚£è€…ä¸ºæˆäººä¸”æœ‰æ‰‹æœ¯åŒ»å˜±ä¸”æœ¯å‰æœ‰ç±»ä¼¼ç„¦è™‘çš„è¯Šæ–­æˆ–å¹¶ç—…ç¨‹è®°å½•æ‰ç®—æ»¡è¶³æ¡ä»¶ã€‚
    åˆå¦‚æº´èŠ¬é…¸é’ çš„æŠ¥é”€æ¡ä»¶ä¸ºï¼šé™çœ¼éƒ¨æ‰‹æœ¯åç‚ç—‡ã€‚å¦‚æœæ‚£è€…ä¿¡æ¯ä¸­æ²¡æœ‰çœ¼éƒ¨æ‰‹æœ¯çš„è®°å½•æˆ–æ²¡æœ‰çœ¼éƒ¨æ‰‹æœ¯åç‚ç—‡çš„ä¿¡æ¯ï¼Œåˆ™æœªæ»¡è¶³æŠ¥é”€æ¡ä»¶ã€‚
2. å¯¹äºä¸€ç§è¢«é—®åˆ°çš„è¯ç‰©ï¼Œå¦‚æœçŸ¥è¯†åº“ä¸­æ²¡æœ‰æ˜ç¡®çš„åŒ»ä¿æŠ¥é”€çº¦æŸï¼Œåˆ™åœ¨è§„åˆ™ä¸­è¯´æ˜çŸ¥è¯†åº“ä¸­æ²¡æœ‰æŠ¥é”€è§„åˆ™ã€‚ä½†ä½ éœ€è¦æ ¹æ®åŒ»ä¿çš„æ™®é€šåŸåˆ™ï¼š
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
åŒ»ä¿æ”¯ä»˜æ”¿ç­–é€šå¸¸åŒ…æ‹¬ï¼š
ç±»å‹	è¯´æ˜
æ— åŒ»ä¿é™åˆ¶	å¯æŠ¥é”€ï¼Œä½†å¿…é¡»ç¬¦åˆä¸´åºŠé€‚åº”ç—‡
é™é€‚åº”ç—‡æŠ¥é”€	è‹¥æ— æ˜ç¡®æ„ŸæŸ“è¯æ®ï¼Œå³ä½¿æ— é™åˆ¶ï¼Œä¹Ÿå¯èƒ½è¢«è®¤å®šä¸ºâ€œä¸åˆç†ç”¨è¯â€è€Œæ‹’ä»˜
é™äºŒçº¿ç”¨è¯	å¿…é¡»åœ¨ä¸€çº¿æŠ—ç”Ÿç´ æ— æ•ˆæˆ–ç¦å¿Œæ—¶ä½¿ç”¨ï¼Œå¦åˆ™å¯èƒ½æ‹’ä»˜
é™ç‰¹å®šäººç¾¤	å¦‚ä»…é™å„¿ç«¥ã€å­•å¦‡ã€HIVæ‚£è€…ç­‰
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
åˆ¤æ–­æœ‰æ²¡æœ‰æ‹’ä»˜é£é™©ã€‚

ç»“åˆæ‚£è€…çš„å®é™…ç—…æƒ…åˆ¤æ–­ç”¨è¯çš„é€‚åº”ç—‡æ˜¯å¦å­˜åœ¨ã€‚å¦‚æœæ‚£è€…ç—…æƒ…ä¸­å­˜åœ¨é€‚åº”ç—‡ï¼Œåˆ™æ²¡æœ‰åŒ»ä¿æ‹’ä»˜é£é™©ï¼›å¦‚æœæ‚£è€…ç—…æƒ…ä¸­æœªå‘ç°é€‚åº”ç—‡ï¼Œåˆ™å¿…é¡»åˆ¤å®šä¸ºæœ‰åŒ»ä¿æ‹’ä»˜é£é™©ã€‚
3. å¯¹äºäºŒçº¿ç”¨è¯ï¼Œå¦‚æœæ‚£è€…ç—…æƒ…ä¸­æ²¡æœ‰ä¸€çº¿ç”¨è¯è®°å½•æˆ–æ²¡æœ‰ä¸€çº¿ç”¨è¯æ— æ•ˆçš„è®°å½•ï¼Œä¹Ÿåº”åˆ¤æ–­ä¸ºæ²¡æœ‰é€‚åº”ç—‡ã€‚åœ¨è§£é‡Šéƒ¨åˆ†åº”è¯¥è¯´æ˜å¦‚æœè¦ç”¨è¿™ç§äºŒçº¿è¯ç‰©ï¼Œå¯¹åº”çš„ä¸€çº¿è¯ç‰©æ˜¯ä»€ä¹ˆï¼ˆä¸¾ä¾‹ï¼‰ã€‚
4. åªè¦æœ‰ä¸€ä¸ªè¯ç‰©å­˜åœ¨æ‹’ä»˜é£é™©ï¼Œåˆ™æ•´ä½“æ¥çœ‹å°±æœ‰æ‹’ä»˜é£é™©ã€‚

å›ç­”æ—¶è¦ç®€æ´ï¼ŒæŒ‰é¡ºåºåŒ…æ‹¬ä¸‰ä¸ªæ–¹é¢å†…å®¹ï¼š
1. ç»“è®ºï¼šæ ¹æ®å½“å‰ä¸Šä¸‹æ–‡ä¸­çš„ä¿¡æ¯æ˜ç¡®å›ç­”æ˜¯å¦æœ‰åŒ»ä¿æ‹’ä»˜é£é™©ï¼Œåªå›ç­”æœ‰æ²¡æœ‰é£é™©å³å¯ã€‚
2. è§„åˆ™ï¼šé’ˆå¯¹é—®é¢˜ä¸­è¢«è¯¢é—®çš„è¯ç‰©ï¼Œè€Œä¸æ˜¯åŒ»ä¿è§„åˆ™ä¿¡æ¯ä¸­çš„è¯ç‰©ï¼Œé€æ¡è§£é‡ŠåŒ»ä¿è§„åˆ™æ˜¯ä»€ä¹ˆæ ·çš„ã€‚åœ¨è¯´æ˜åŒ»ä¿è§„åˆ™æ—¶åº”ä¸¥æ ¼å¼•ç”¨ä¸Šä¸‹æ–‡ä¸­è®°å½•çš„è§„åˆ™æ–‡æœ¬ï¼Œä¸è¦åˆ›é€ å†…å®¹ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰è¿”å›å¯¹åº”çš„è¯ç‰©çš„ä¿¡æ¯ï¼Œå¯ä»¥å½“ä½œæ²¡æœ‰ç‰¹å®šçš„åŒ»ä¿è§„åˆ™ï¼Œä¸è¦åˆ—å‡ºå…¶å®ƒè¯ç‰©çš„åŒ»ä¿è§„åˆ™ã€‚ç¦æ­¢åˆ—å‡ºæ²¡è¢«é—®åˆ°çš„è¯å“çš„ä¿¡æ¯ã€‚l
3. è§£é‡Šï¼šé’ˆå¯¹é—®é¢˜ä¸­è¢«é—®åˆ°çš„æ¯ä¸€ç§è¯ç‰©ï¼Œå¦‚æœæœ‰æ‹’ä»˜é£é™©ï¼Œè¯´æ˜åŸå› ã€‚å¦‚æœæ²¡æœ‰æ‹’ä»˜é£é™©ï¼Œè¯´æ˜æ”¯æ’‘æ¡ä»¶æ˜¯ä»€ä¹ˆã€‚ä¸è¦åˆ—å‡ºæ²¡æœ‰è¢«é—®åˆ°çš„è¯ç‰©çš„ä¿¡æ¯ã€‚
"""

# å®šä¹‰ä¸€ä¸ªç”¨äºæ£€æµ‹é™éŸ³çš„é˜ˆå€¼å’Œä¸€ä¸ªç”¨äºç»“æŸä¸€è½®ï¼ˆå¯¹è¯æˆ–äº¤äº’ï¼‰çš„è¶…æ—¶æ—¶é—´ã€‚
SILENCE_THRESHOLD = (
    2000  # Adjust based on your audio level (e.g., lower for quieter audio)
)
SILENCE_TIMEOUT = 2000.0  # Seconds of silence to consider the turn finished

# åˆå§‹åŒ–IRISä¸Šä¸‹æ–‡ç®¡ç†å™¨
ctx = IRISContextManager(
    host=os.getenv("IRIS_HOSTNAME"),
    port=int(os.getenv("IRIS_PORT")),
    namespace=os.getenv("IRIS_NAMESPACE"),
    username=os.getenv("IRIS_USERNAME"),
    password=os.getenv("IRIS_PASSWORD"),
)

# LLMå®¢æˆ·ç«¯
client = AsyncOpenAI(
    api_key=os.getenv("DEEPSEEK_API_KEY"), 
    base_url="https://api.deepseek.com/v1"
)

# éŸ³é¢‘ç†è§£ï¼ˆQwen-Audioï¼‰å®¢æˆ·ç«¯ï¼ˆç”±äºQwenä¸å…¼å®¹OpenAIéŸ³é¢‘æ¥å£ï¼Œéœ€é¢å¤–æ„å»ºå®¢æˆ·ç«¯ï¼‰
dashscope.api_key = os.getenv('DASHSCOPE_API_KEY')
#.AsyncClient()

# å„ç±»å·¥å…·å‡½æ•°
def get_or_create_session(cl, ctx):
    session_id = cl.user_session.get("session_id")
    if not session_id:
        session_id = str(uuid.uuid4())
        cl.user_session.set("session_id", session_id)
    # åˆ¤æ–­IRISæ˜¯å¦å·²æœ‰æ­¤sessionï¼Œæ²¡æœ‰æ‰åˆ›å»º
    if ctx.get_session(session_id) is None:
        ctx.create_session(session_id)  # metaå‚æ•°å¯ä»¥çœç•¥
        # åˆ›å»ºsessionæ—¶ç»‘å®šåŒ»ç”Ÿèº«ä»½
        save_assistant_message(ctx,session_id,f"æˆ‘æ˜¯ä¸€ä¸ªä¸´åºŠåŒ»ç”Ÿçš„é—¨è¯ŠåŠ©æ‰‹ã€‚ç°åœ¨ç™»å½•çš„ä¸´åºŠåŒ»ç”Ÿçš„èµ„æºidæ˜¯{prac_id},ä»–çš„å§“åæ˜¯{prac_name}ã€‚æˆ‘ä»¬ç”¨ä¸­æ–‡äº¤æµã€‚")
    return session_id

def save_user_message(ctx, session_id, msg):
    ctx.append_history(session_id, "user", msg.content)

def get_history_str(ctx, session_id):
    history = ctx.get_history(session_id)
    return "\n".join([f"{item['role']}: {item['content']}" for item in history]), history

def build_tool_descriptions(mcp_tools):
    if not mcp_tools:
        return ""
    tool_list = list(mcp_tools.values())[0]
    descriptions = []
    for t in tool_list:
        input_schema = t.get('input_schema', {})
        if isinstance(input_schema, dict):
            input_fields = ", ".join([
                f"{k}({(v.get('type') if isinstance(v, dict) else v) or 'æœªçŸ¥ç±»å‹'})"
                for k, v in input_schema.items()
            ]) if input_schema else "æ— è¾“å…¥å­—æ®µ"
        else:
            input_fields = "æœªçŸ¥è¾“å…¥ç»“æ„"
        descriptions.append(f"- {t['name']}: {t['description']} | è¾“å…¥å­—æ®µ: {input_fields}")
    return "\n".join(descriptions)

async def call_mcp_tool(session, tool_name, tool_input):
    try:
        cl.logger.info(f"åŸºäºè¾“å…¥ {tool_input} è°ƒç”¨å·¥å…· {tool_name} ")
        result = await session.call_tool(tool_name, tool_input)
        return str(result)
    except Exception as e:
        return f"è°ƒç”¨ MCP å·¥å…·å¤±è´¥: {e}"

def save_assistant_message(ctx, session_id, answer):
    ctx.append_history(session_id, "assistant", answer)

async def send_messages(cl, answer, reasoning_output, counter):
    await cl.Message(content=answer).send()
    await cl.Message(content=f"ğŸ“ æ¨ç†ä¸è°ƒç”¨è¿‡ç¨‹:\n{reasoning_output}").send()
    await cl.Message(content=f"ä½ å·²ç»å‘é€äº† {counter} æ¡æ¶ˆæ¯ï¼").send()


# è¯­éŸ³è½¬æ–‡å­—
async def speech_to_text(audio_buffer):
    base64_audio = base64.b64encode(audio_buffer).decode('utf-8')
    messages = [
            {
                "role": "user",
                "content": [
                    # éŸ³é¢‘å†…å®¹ï¼Œä½¿ç”¨base64ç¼–ç 
                    {"audio": f"data:audio/wav;base64,{base64_audio}"},
                    {"text": "è¯·å°†è¿™æ®µè¯­éŸ³è½¬æ¢ä¸ºä¸­æ–‡æ–‡å­—ï¼Œä»…è¾“å‡ºè½¬æ¢åçš„å†…å®¹ï¼Œä¸æ·»åŠ ä»»ä½•å‰ç¼€ã€åç¼€æˆ–è§£é‡Šè¯´æ˜ï¼Œç›´æ¥è¿”å›åŸå§‹æ–‡æœ¬ã€‚"}
                ]
            }
        ]
    try:
            # è°ƒç”¨é€šæ„åƒé—®çš„éŸ³é¢‘æ¨¡å‹
            response = MultiModalConversation.call(
                model='qwen2-audio-instruct',  # ä½¿ç”¨éŸ³é¢‘ä¸“ç”¨æ¨¡å‹
                messages=messages
            )
            # å¤„ç†APIå“åº”
            if response.status_code == 200 and response.output:
                content = response.output.choices[0].message.content
                print(f"è¯­éŸ³è½¬æ–‡å­—ç»“æœ: {content}")
            if content:  # å…ˆåˆ¤æ–­ç»“æœä¸ä¸ºç©º
            # æå–åˆ—è¡¨ä¸­ç¬¬ä¸€ä¸ªå­—å…¸çš„ 'text' å­—æ®µå€¼
                extracted_text = content[0]['text']
                print(f"æå–åçš„æ–‡æœ¬: {extracted_text}")
                return extracted_text
            else:
                print(f"APIè°ƒç”¨å¤±è´¥: {response.message}")
                return None
    except Exception as e:
            print(f"å¤„ç†éŸ³é¢‘æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            return None


# chainlit äº‹ä»¶é’©å­
@cl.on_chat_start
async def on_chat_start():
    # æ¯æ¬¡æ–°ä¼šè¯åˆ†é…ä¸€ä¸ªsession_idå¹¶åˆ›å»ºglobal
    #session_id = str(uuid.uuid4())
    session_id = get_or_create_session(cl, ctx)
    cl.user_session.set("session_id", session_id)
    cl.user_session.set("counter", 0)
    cl.user_session.set("temp_values",{})
    cl.logger.info(f"åŒ»ç”Ÿ{prac_id}çš„åå­—æ˜¯{prac_name}")
    #cl.user_session.set("practitioner",)
    cl.logger.info(f"æ–°ä¼šè¯åˆ†é… session_id: {session_id}")
    initMsg = f"æ¬¢è¿æ‚¨ï¼Œ{prac_name}åŒ»ç”Ÿï¼Œæˆ‘æ˜¯æ‚¨çš„é—¨è¯ŠåŠ©æ‰‹{assistant_name}ã€‚æˆ‘ä¼šååŠ©æ‚¨å®Œæˆé—¨è¯Šï¼Œæ¬¢è¿æ‚¨å‘æˆ‘æå‡ºä»»ä½•é—®é¢˜ã€‚"
    await cl.Message(
        content=initMsg
    ).send()
    save_assistant_message(ctx, session_id, initMsg)

@cl.on_mcp_connect
async def on_mcp_connect(connection, session: ClientSession):
    cl.logger.info(f"Chainlit æ­£åœ¨å°è¯•è¿æ¥ MCP Server: {connection.name}")
    try:
        result = await session.list_tools()
        tools = [{"name": t.name, "description": t.description, "input_schema": t.inputSchema} for t in result.tools]
        cl.user_session.set("mcp_tools", {connection.name: tools})
        cl.user_session.set("mcp_session", session)
        cl.logger.info(f"MCP è¿æ¥æˆåŠŸï¼Œå·²è·å– {len(tools)} ä¸ªå·¥å…·")
        
        cl.logger.info(json.dumps(tools,indent=2,ensure_ascii=False))
    except Exception as e:
        cl.logger.error(f"è·å– MCP å·¥å…·åˆ—è¡¨æ—¶å‡ºé”™: {e}")


# ===================
# ä¸» on_message æ–¹æ³•
# ===================

@cl.on_message
async def on_message(msg: cl.Message):
    session_id = get_or_create_session(cl, ctx)
    #save_user_message(ctx, session_id, msg)
    history_str, history = get_history_str(ctx, session_id)
    original_quest = msg.content

    # === ä¸Šä¸‹æ–‡ä¼˜å…ˆåˆ¤æ–­ ===
    # å…ˆåˆ¤æ–­ä¸Šä¸‹æ–‡ä¸­çš„æ•°æ®æ˜¯å¦è¶³å¤Ÿå›ç­”é—®é¢˜
    can_answer, reasoning = await can_answer_from_context(history, msg.content, client,llm_model)
    #cl.logger.info(f"å†å²ä¿¡æ¯1ï¼š {history_str}")
    #cl.logger.info(f"å†å²ä¿¡æ¯2ï¼š {history}")
    #cl.logger.info(f"åˆ¤æ–­çŠ¶æ€ï¼š {can_answer}, åŸå› æ˜¯: {reasoning}")
    save_user_message(ctx, session_id, msg)
    # ç”¨Stepæ˜¾ç¤ºåˆ¤æ–­ç»“æœ    
    async with cl.Step("åˆ¤æ–­ç»“æœæ˜¾ç¤º") as step:
        step.output = f"ğŸ§  ä¸Šä¸‹æ–‡åˆ¤æ–­: {'å¯ä»¥' if can_answer else 'ä¸å¯ä»¥'}ç›´æ¥å›ç­”\n"+ f"ğŸ“ åˆ¤æ–­ç†ç”±: {reasoning}"

    if can_answer:
        # ç›´æ¥åŸºäºä¸Šä¸‹æ–‡ç”Ÿæˆå›ç­”
        answer = await generate_context_answer(history, msg.content, client, llm_model)
        visual_tag = 'éœ€è¦å›¾è¡¨'
        need_visual = False
        # ä¿å­˜å¹¶è¿”å›å›ç­”
        if visual_tag in answer:
            answer = answer.replace(visual_tag, "")
            need_visual = True
        save_assistant_message(ctx, session_id, answer)
        await cl.Message(content=answer).send()
        # è°ƒç”¨Agentç»˜å›¾
        if need_visual:
            print("å‡†å¤‡ç”»å›¾")
            await generate_interactive_plotly_chart(answer,original_quest,client, llm_model)

        return  # ç»“æŸå¤„ç†ï¼Œä¸å†æ‰§è¡Œåç»­å·¥å…·è°ƒç”¨

    mcp_tools = cl.user_session.get("mcp_tools")
    session = cl.user_session.get("mcp_session")
    # === è°ƒç”¨ planner_agent ç”Ÿæˆ plan ===
    # æå–åŸå§‹tools
    tool_list = []
    if mcp_tools:
        for v in mcp_tools.values():
            tool_list.extend(v)

    # ç”Ÿæˆå¤šæ­¥ plan
    plan_json = await generate_plan(history, msg.content, tool_list,client,llm_model)
    raw_content = plan_json.get("raw", [])
    json_str = raw_content.strip().replace('```json', '').replace('```', '').strip()
    json_data = json.loads(json_str)
    plan = json_data.get("plan", [])
    explanation = json_data.get("explanation", "")

    print("????????")
    print(plan_json)
    process_steps = [f"å¤šæ­¥æ‰§è¡Œè®¡åˆ’ï¼š{json.dumps(plan, ensure_ascii=False, indent=2)}", f"è®¡åˆ’è¯´æ˜ï¼š{explanation}"]
    print(process_steps)
    answer_texts = []

    temp_values = cl.user_session.get("temp_values")

    for idx, step in enumerate(plan):
        action = step.get("action")
        tool = step.get("tool")
        input_data = step.get("input")
        result_var = step.get("result_var")
        step_desc = step.get("description", "")
        process_steps.append(f"Step {idx+1}: {step_desc}")
        #cl.logger.info(input_data)
        # å¦‚æœinput_dataæ˜¯Dictç±»å‹ï¼Œåˆ™è¡¨æ˜å…¶ä¸­åŒ…å«äº†ä¸Šä¸‹æ–‡ä¸­ä¿å­˜çš„ä¸´æ—¶å˜é‡ï¼Œéœ€ä»temp_valuesä¸­æå–å¯¹åº”çš„ä¸´æ—¶å˜é‡æ›¿æ¢å…¶å€¼
        if isinstance(input_data, dict):
            for key in input_data:
                    if isinstance(input_data[key], str) and input_data[key] in temp_values:
                        input_data[key] = temp_values[input_data[key]]
        #cl.logger.info(temp_values)
        #cl.logger.info(input_data)
        if action == "call_tool" and tool and session:
            # è°ƒç”¨ MCP å·¥å…·
            try:
                raw_result = await session.call_tool(tool, input_data)
                parsed_contents = parse_mcp_result(raw_result)
                #cl.logger.info(parsed_contents)
                # å¦‚æœresult_varä¸­æ ‡è¯†å‡ºäº†ä¸´æ—¶å˜é‡çš„åç§°ï¼Œåˆ™å°†parsed_contentsä½œä¸ºä¸´æ—¶å˜é‡å€¼èµ‹ç»™è¿™ä¸ªä¸´æ—¶å˜é‡
                if result_var:
                    temp_values[result_var]=get_result_value(parsed_contents)
                for content_type, content_value in parsed_contents:
                    if content_type == "text":
                        answer_texts.append(content_value)
                        #æ‰“å°å·¥å…·è¿”å›ç»“æœ
                        async with cl.Step(name="å·¥å…·è¿”å›ç»“æœ") as step:
                            step.output = content_value
                        #await cl.Message(content=f"[{tool}] {content_value}").send()
                    elif content_type == "file":
                        await cl.Message(content=f"æ–‡ä»¶é“¾æ¥: {content_value}").send()
                    elif content_type == "image":
                        await cl.Message(content="æ”¶åˆ°å›¾ç‰‡:").send()
                        await cl.Message(image=content_value).send()
                    elif content_type == "error":
                        answer_texts.append(f"âŒ {content_value}")
                        await cl.Message(content=f"âŒ {content_value}").send()
                    else:
                        await cl.Message(content=f"å…¶ä»–å†…å®¹: {content_value}").send()
            except Exception as e:
                err_msg = f"è°ƒç”¨å·¥å…· {tool} å¤±è´¥: {e}"
                answer_texts.append(err_msg)
                await cl.Message(content=err_msg).send()
        elif action == "llm_answer":
            # ç›´æ¥ç”¨LLMè‡ªèº«èƒ½åŠ›ç”Ÿæˆå›ç­”
            #cl.logger.info(temp_values)
            llm_prompt = input_data if isinstance(input_data, str) else str(input_data)
            message = cl.Message(content="")
            stream = await client.chat.completions.create(
                model=llm_model,
                messages=[
                    {"role": "system", "content": assistant_prompt},
                    {"role": "user", "content": llm_prompt},
                ],
                stream=True,
                temperature=0.01
            )
            async for part in stream:
                delta = part.choices[0].delta
                if delta.content:
                    # Stream the output of the step
                    await message.stream_token(delta.content)
            answer_texts.append(message.content)
        # åŒ»ä¿é£é™©åˆ†æ
        elif action == "risk_analyst":
            context_data = input_data if isinstance(input_data, str) else str(input_data)
            user_question = step_desc if isinstance(step_desc, str) else str(step_desc)
            question_prompt = f"""
            é—®é¢˜ï¼š{user_question}
            ä¸Šä¸‹æ–‡ä¿¡æ¯:{context_data}
            """
            message = cl.Message(content="")
            stream = await client.chat.completions.create(
                model=llm_model,
                messages=[
                    {"role": "system", "content": insurance_expert_prompt},
                    {"role": "user", "content": question_prompt},
                ],
                stream=True,
                temperature=0.01
            )
            async for part in stream:
                delta = part.choices[0].delta
                if delta.content:
                    # Stream the output of the step
                    await message.stream_token(delta.content)
            answer_texts.append(message.content)
        else:
            # è®¡åˆ’æ ¼å¼ä¸å¯¹
            process_steps.append(f"æ— æ³•è¯†åˆ«çš„è®¡åˆ’ç±»å‹ï¼š{step}")

    # æ±‡æ€»æœ¬è½®å¯¹è¯å†…å®¹å¹¶ä¿å­˜åœ¨IRISä¸­
    answer = "\n".join(answer_texts)
    if answer:
        save_assistant_message(ctx, session_id, answer)
    counter = cl.user_session.get("counter")
    counter += 1
    cl.user_session.set("counter", counter)
    process_steps.append(f"ä½ å·²ç»å‘é€äº† {counter} æ¡æ¶ˆæ¯ï¼")
    #await cl.Message(content="ğŸ“ æœ¬è½®å¤šæ­¥æ¨ç†/æ‰§è¡Œè¿‡ç¨‹ï¼š\n" + "\n".join(process_steps)).send()

@cl.on_audio_start
async def on_audio_start():
    cl.user_session.set("silent_duration_ms", 0)
    cl.user_session.set("is_speaking", False)
    cl.user_session.set("audio_chunks", [])
    return True

@cl.on_audio_end
async def on_audio_end():
    await process_audio()
    return True

@cl.on_audio_chunk
async def on_audio_chunk(chunk: cl.InputAudioChunk):
    audio_chunks = cl.user_session.get("audio_chunks")

    if audio_chunks is not None:
        audio_chunk = np.frombuffer(chunk.data, dtype=np.int16)
        audio_chunks.append(audio_chunk)

    # If this is the first chunk, initialize timers and state
    if chunk.isStart:
        cl.user_session.set("last_elapsed_time", chunk.elapsedTime)
        cl.user_session.set("is_speaking", True)
        return

    audio_chunks = cl.user_session.get("audio_chunks")
    last_elapsed_time = cl.user_session.get("last_elapsed_time")
    silent_duration_ms = cl.user_session.get("silent_duration_ms")
    is_speaking = cl.user_session.get("is_speaking")

    # Calculate the time difference between this chunk and the previous one
    time_diff_ms = chunk.elapsedTime - last_elapsed_time
    cl.user_session.set("last_elapsed_time", chunk.elapsedTime)

    # Compute the RMS (root mean square) energy of the audio chunk
    audio_energy = audioop.rms(
        chunk.data, 2
    )  # Assumes 16-bit audio (2 bytes per sample)

    if audio_energy < SILENCE_THRESHOLD:
        # Audio is considered silent
        silent_duration_ms += time_diff_ms
        cl.user_session.set("silent_duration_ms", silent_duration_ms)
        if silent_duration_ms >= SILENCE_TIMEOUT and is_speaking:
            cl.user_session.set("is_speaking", False)
            #await process_audio() 
    else:
        # Audio is not silent, reset silence timer and mark as speaking
        cl.user_session.set("silent_duration_ms", 0)
        if not is_speaking:
            cl.user_session.set("is_speaking", True)

async def process_audio():
    # Get the audio buffer from the session
    if audio_chunks := cl.user_session.get("audio_chunks"):
        # Concatenate all chunks
        concatenated = np.concatenate(list(audio_chunks))

        # Create an in-memory binary stream
        wav_buffer = io.BytesIO()

        # Create WAV file with proper parameters
        with wave.open(wav_buffer, "wb") as wav_file:
            wav_file.setnchannels(1)  # mono
            wav_file.setsampwidth(2)  # 2 bytes per sample (16-bit)
            wav_file.setframerate(24000)  # sample rate (24kHz PCM)
            wav_file.writeframes(concatenated.tobytes())

        # Reset buffer position
        wav_buffer.seek(0)

        cl.user_session.set("audio_chunks", [])

    frames = wav_file.getnframes()
    rate = wav_file.getframerate()

    duration = frames / float(rate)
    if duration <= 1.50:
        print("The audio is too short, please try again.")
        return

    audio_buffer = wav_buffer.getvalue()

    input_audio_el = cl.Audio(content=audio_buffer, mime="audio/wav")
    #sound_input = ("audio.wav", audio_buffer, "audio/wav") 
       
    transcription = await speech_to_text(audio_buffer)

    async with cl.Step("å°†è¯­éŸ³è½¬ä¸ºæ–‡å­—") as step:
        step.output = f"ğŸ§  ç”¨æˆ·çš„é—®é¢˜æ˜¯: {transcription}"
        step.elements = [input_audio_el]

    msg = cl.Message(content=f"ğŸ§  ç”¨æˆ·çš„é—®é¢˜æ˜¯: {transcription}")
    await msg.send()
    await on_message(msg)
